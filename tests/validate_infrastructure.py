#!/usr/bin/env python3
"""
Final Validation Script - Verify Complete Testing Infrastructure
Checks all test files, validates syntax, ensures all tests are ready
"""

import json
import os
import sys
from datetime import datetime
from pathlib import Path


class FinalValidator:
    """Final validation of testing infrastructure."""

    def __init__(self):
        self.workspace = Path(__file__).parent.parent
        self.tests_dir = Path(__file__).parent  # Current directory (tests/)
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "checks": {},
            "all_passed": True,
        }

    def check_test_files_exist(self):
        """Check 1: All test files exist."""
        test_name = "Test Files Exist"
        print(f"\nüîç {test_name}")

        required_files = [
            "test_unit_comprehensive.py",
            "test_multiplatform_validation.py",
            "test_e2e_playwright.py",
            "test_e2e_complete.py",
            "run_all_tests.py",
            "TESTING_GUIDE.md",
            "__init__.py",
        ]

        all_exist = True
        for filename in required_files:
            filepath = self.tests_dir / filename
            exists = filepath.exists()
            status = "‚úÖ" if exists else "‚ùå"
            print(f"  {status} {filename}")

            if not exists:
                all_exist = False
                self.results["all_passed"] = False

        self.results["checks"][test_name] = {
            "status": "passed" if all_exist else "failed",
            "files_checked": len(required_files),
            "files_found": sum(
                1 for f in required_files if (self.tests_dir / f).exists()
            ),
        }

        return all_exist

    def check_syntax(self):
        """Check 2: Python files have valid syntax."""
        test_name = "Python Syntax Valid"
        print(f"\nüîç {test_name}")

        python_files = [
            "test_unit_comprehensive.py",
            "test_multiplatform_validation.py",
            "test_e2e_playwright.py",
            "run_all_tests.py",
        ]

        all_valid = True
        for filename in python_files:
            filepath = self.tests_dir / filename

            try:
                with open(filepath) as f:
                    compile(f.read(), filename, "exec")
                print(f"  ‚úÖ {filename}")
            except SyntaxError as e:
                print(f"  ‚ùå {filename}: {e}")
                all_valid = False
                self.results["all_passed"] = False
            except Exception as e:
                print(f"  ‚ö†Ô∏è  {filename}: {e}")

        self.results["checks"][test_name] = {
            "status": "passed" if all_valid else "failed",
            "files_checked": len(python_files),
            "syntax_valid": sum(1 for f in python_files if self._check_file_syntax(f)),
        }

        return all_valid

    def _check_file_syntax(self, filename):
        """Helper to check file syntax."""
        try:
            filepath = self.tests_dir / filename
            with open(filepath) as f:
                compile(f.read(), filename, "exec")
            return True
        except:
            return False

    def check_test_count(self):
        """Check 3: Test count meets minimum."""
        test_name = "Test Count"
        print(f"\nüîç {test_name}")

        # Count tests
        min_tests = {"unit": 16, "multiplatform": 12, "e2e": 10}

        results = {}
        all_meet_minimum = True

        for test_type, minimum in min_tests.items():
            # Approximate by counting test definitions
            file_map = {
                "unit": "test_unit_comprehensive.py",
                "multiplatform": "test_multiplatform_validation.py",
                "e2e": "test_e2e_playwright.py",
            }

            filepath = self.tests_dir / file_map[test_type]

            with open(filepath) as f:
                content = f.read()
                count = content.count("def test_")

            meets = count >= minimum
            status = "‚úÖ" if meets else "‚ö†Ô∏è"
            print(f"  {status} {test_type.upper()}: {count} tests (min: {minimum})")

            results[test_type] = count
            if not meets:
                all_meet_minimum = False

        self.results["checks"][test_name] = {
            "status": "passed" if all_meet_minimum else "warning",
            "unit_tests": results.get("unit", 0),
            "multiplatform_tests": results.get("multiplatform", 0),
            "e2e_tests": results.get("e2e", 0),
            "total": sum(results.values()),
        }

        return all_meet_minimum

    def check_documentation(self):
        """Check 4: Documentation exists."""
        test_name = "Documentation"
        print(f"\nüîç {test_name}")

        docs_files = [
            ("tests/TESTING_GUIDE.md", "Testing guide"),
            ("COMPREHENSIVE_TEST_REPORT.md", "Test report"),
        ]

        all_exist = True
        for rel_path, desc in docs_files:
            filepath = self.workspace / rel_path
            exists = filepath.exists()
            status = "‚úÖ" if exists else "‚ùå"
            print(f"  {status} {desc}: {rel_path}")

            if not exists:
                all_exist = False

        self.results["checks"][test_name] = {
            "status": "passed" if all_exist else "failed",
            "docs_found": sum(
                1 for path, _ in docs_files if (self.workspace / path).exists()
            ),
            "docs_required": len(docs_files),
        }

        return all_exist

    def check_imports(self):
        """Check 5: Required imports available."""
        test_name = "Required Imports"
        print(f"\nüîç {test_name}")

        imports_to_check = [
            ("json", "Built-in"),
            ("pathlib.Path", "Built-in"),
            ("datetime", "Built-in"),
            ("xml.etree.ElementTree", "Built-in"),
            ("csv", "Built-in"),
        ]

        available = 0
        for import_name, category in imports_to_check:
            try:
                if import_name == "pathlib.Path":
                    from pathlib import Path
                elif import_name == "xml.etree.ElementTree":
                    import xml.etree.ElementTree
                else:
                    parts = import_name.split(".")
                    __import__(import_name)

                print(f"  ‚úÖ {import_name} ({category})")
                available += 1
            except ImportError:
                print(f"  ‚ùå {import_name} ({category})")

        self.results["checks"][test_name] = {
            "status": "passed" if available == len(imports_to_check) else "warning",
            "available": available,
            "required": len(imports_to_check),
        }

        return available == len(imports_to_check)

    def check_master_runner(self):
        """Check 6: Master test runner configured."""
        test_name = "Master Test Runner"
        print(f"\nüîç {test_name}")

        runner_file = self.tests_dir / "run_all_tests.py"

        with open(runner_file) as f:
            content = f.read()

        checks = [
            ("run_unit_tests", "Unit test execution"),
            ("run_multiplatform_tests", "Multi-platform test execution"),
            ("run_e2e_tests", "E2E test execution"),
            ("calculate_coverage", "Coverage calculation"),
            ("generate_report", "Report generation"),
        ]

        all_present = True
        for method_name, description in checks:
            present = f"def {method_name}" in content
            status = "‚úÖ" if present else "‚ùå"
            print(f"  {status} {description}")

            if not present:
                all_present = False

        self.results["checks"][test_name] = {
            "status": "passed" if all_present else "failed",
            "methods_required": len(checks),
            "methods_found": sum(
                1 for method, _ in checks if f"def {method}" in content
            ),
        }

        return all_present

    def check_coverage_targets(self):
        """Check 7: Coverage targets configured."""
        test_name = "Coverage Targets"
        print(f"\nüîç {test_name}")

        runner_file = self.tests_dir / "run_all_tests.py"

        with open(runner_file) as f:
            content = f.read()

        coverage_modules = [
            "data_processing",
            "web_interface",
            "pdf_generation",
            "markdown_processing",
            "api_endpoints",
            "multi_platform",
        ]

        configured = 0
        for module in coverage_modules:
            in_content = f'"{module}"' in content or f"'{module}'" in content
            status = "‚úÖ" if in_content else "‚ö†Ô∏è"
            print(f"  {status} {module}")

            if in_content:
                configured += 1

        self.results["checks"][test_name] = {
            "status": "passed",
            "modules_configured": configured,
            "modules_required": len(coverage_modules),
        }

        return True

    def check_error_handling(self):
        """Check 8: Error handling in tests."""
        test_name = "Error Handling"
        print(f"\nüîç {test_name}")

        test_files = [
            "test_unit_comprehensive.py",
            "test_multiplatform_validation.py",
            "test_e2e_playwright.py",
        ]

        all_have_handling = True
        for filename in test_files:
            filepath = self.tests_dir / filename

            with open(filepath) as f:
                content = f.read()

            has_try_except = "try:" in content and "except" in content
            has_assertions = "assert" in content

            status = "‚úÖ" if (has_try_except or has_assertions) else "‚ö†Ô∏è"
            print(f"  {status} {filename}: Error handling present")

            if not (has_try_except or has_assertions):
                all_have_handling = False

        self.results["checks"][test_name] = {
            "status": "passed" if all_have_handling else "warning",
            "files_checked": len(test_files),
            "files_with_handling": sum(
                1 for f in test_files if self._has_error_handling(f)
            ),
        }

        return all_have_handling

    def _has_error_handling(self, filename):
        """Helper to check for error handling."""
        filepath = self.tests_dir / filename
        with open(filepath) as f:
            content = f.read()
        return ("try:" in content and "except" in content) or "assert" in content

    def run_all_checks(self):
        """Run all validation checks."""
        print("\n" + "‚ïî" + "=" * 68 + "‚ïó")
        print("‚ïë" + " " * 68 + "‚ïë")
        print(
            "‚ïë" + " COMPREHENSIVE TESTING INFRASTRUCTURE VALIDATION ".center(68) + "‚ïë"
        )
        print("‚ïë" + " " * 68 + "‚ïë")
        print("‚ïö" + "=" * 68 + "‚ïù")

        # Run all checks
        check_results = [
            self.check_test_files_exist(),
            self.check_syntax(),
            self.check_test_count(),
            self.check_documentation(),
            self.check_imports(),
            self.check_master_runner(),
            self.check_coverage_targets(),
            self.check_error_handling(),
        ]

        # Summary
        print("\n" + "=" * 70)
        print("üìä VALIDATION SUMMARY")
        print("=" * 70)

        for check_name, result in self.results["checks"].items():
            status = result.get("status", "unknown").upper()
            if status == "PASSED":
                icon = "‚úÖ"
            elif status == "WARNING":
                icon = "‚ö†Ô∏è"
            else:
                icon = "‚ùå"

            print(f"\n{icon} {check_name}")

            # Print details
            for key, value in result.items():
                if key != "status":
                    print(f"   {key}: {value}")

        # Final status
        print("\n" + "=" * 70)

        if self.results["all_passed"]:
            print("üéâ ALL VALIDATION CHECKS PASSED")
            print("   Testing infrastructure is PRODUCTION READY")
            print("   ‚Üí Run: python3 tests/run_all_tests.py")
        else:
            print("‚ö†Ô∏è  SOME VALIDATION CHECKS FAILED")
            print("   Review failures above and fix issues")

        print("=" * 70 + "\n")

        # Save results
        report_file = self.tests_dir / "validation_report.json"
        with open(report_file, "w") as f:
            json.dump(self.results, f, indent=2)

        print(f"üìÑ Validation report saved to: {report_file}\n")

        return self.results["all_passed"]


def main():
    """Main entry point."""
    validator = FinalValidator()
    success = validator.run_all_checks()
    return 0 if success else 1


if __name__ == "__main__":
    exit(main())
